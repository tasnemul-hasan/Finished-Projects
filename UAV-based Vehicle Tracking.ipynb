{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6368074,"sourceType":"datasetVersion","datasetId":3540471},{"sourceId":8450907,"sourceType":"datasetVersion","datasetId":5036266},{"sourceId":8575832,"sourceType":"datasetVersion","datasetId":5128116}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Building the Boxmot Module","metadata":{}},{"cell_type":"code","source":"!pip install ultralytics\n!git clone https://github.com/KeeganFernandesWork/yolo_tracking\n%cd yolo_tracking\n!pip install -r requirements.txt\n!pip install .\n\nfrom IPython.display import clear_output\nclear_output(wait=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T16:08:42.888740Z","iopub.execute_input":"2024-06-01T16:08:42.889660Z","iopub.status.idle":"2024-06-01T16:09:38.445105Z","shell.execute_reply.started":"2024-06-01T16:08:42.889623Z","shell.execute_reply":"2024-06-01T16:09:38.443880Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nfrom IPython.display import display\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nfrom ultralytics import YOLO\nfrom pathlib import Path\nimport numpy as np\nimport tracemalloc\nimport shutil\nimport time\nimport PIL\nimport cv2\nimport os\nimport sys","metadata":{"_uuid":"8f90280b-8fc1-41d7-b4bf-8c0d88dd5ee0","_cell_guid":"a15ac88f-5cc8-4820-bc93-204560c500ee","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-01T16:09:38.447374Z","iopub.execute_input":"2024-06-01T16:09:38.447730Z","iopub.status.idle":"2024-06-01T16:09:42.269996Z","shell.execute_reply.started":"2024-06-01T16:09:38.447699Z","shell.execute_reply":"2024-06-01T16:09:42.268831Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from boxmot import (OCSORT, BoTSORT, BYTETracker, DeepOCSORT, StrongSORT,create_tracker, get_tracker_config)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T16:09:42.271368Z","iopub.execute_input":"2024-06-01T16:09:42.271948Z","iopub.status.idle":"2024-06-01T16:09:43.333673Z","shell.execute_reply.started":"2024-06-01T16:09:42.271916Z","shell.execute_reply":"2024-06-01T16:09:43.332774Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Video Writer","metadata":{}},{"cell_type":"code","source":"def create_video_writer(video_cap, output_filename):\n\n    # grab the width, height, and fps of the frames in the video stream.\n    frame_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = int(video_cap.get(cv2.CAP_PROP_FPS))\n\n    # initialize the FourCC and a video writer object\n    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n    writer = cv2.VideoWriter(output_filename, fourcc, fps,\n                             (frame_width, frame_height))\n\n    return writer","metadata":{"_uuid":"5b27fb1c-e426-45ef-a2c6-9261f1d6d110","_cell_guid":"0de24502-e2ea-4a5d-990e-71f580f78513","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-01T16:09:43.335747Z","iopub.execute_input":"2024-06-01T16:09:43.336329Z","iopub.status.idle":"2024-06-01T16:09:43.342829Z","shell.execute_reply.started":"2024-06-01T16:09:43.336298Z","shell.execute_reply":"2024-06-01T16:09:43.341693Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"color = (0, 0, 255)  # BGR\nthickness = 10\nfontscale = 2\n\ndevice = \"cuda:0\" # cuda:0 , cpu\nfp16 = True # True if gpu available\n# load the pre-trained YOLOv8n model\n\nt = 4    #test video no\nm = 9    #YOLO model no\n\nout_dir = f'/kaggle/working/Test{t}/Y{m}_T{t}_'\n\nmodel = YOLO(f\"/kaggle/working/Weights/Yolo{m}_best.pt\")\nsource = f\"/kaggle/input/test-video-15fps/Test4_15fps.mp4\"\n\n#target_obj_img = np.array([]) # if we have no target and we want to track all objects\ntarget_obj_img = cv2.imread(f'/kaggle/input/videos-target/Test{t}Target.jpg') ","metadata":{"_uuid":"7b034df8-bce5-48d8-9884-9aeb9846eecb","_cell_guid":"e7a9dabf-7a39-4a72-aa4b-7789d036c7f1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-01T16:09:43.344167Z","iopub.execute_input":"2024-06-01T16:09:43.344479Z","iopub.status.idle":"2024-06-01T16:09:43.613422Z","shell.execute_reply.started":"2024-06-01T16:09:43.344453Z","shell.execute_reply":"2024-06-01T16:09:43.612547Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# SIFT Feature Matching","metadata":{}},{"cell_type":"code","source":"def crop(frame, coords, save, obj_name):\n    frame_np_to_PIL = Image.fromarray(frame)        # change to PIL format for easy croping\n    cropped_obj_PIL = frame_np_to_PIL.crop(coords)  # do crop\n    cropped_obj_to_np = np.asarray(cropped_obj_PIL) # convert to cv2 format\n    cropped_obj_rgb = cv2.cvtColor(cropped_obj_to_np, cv2.COLOR_BGR2RGB)  # PIL was BGR format, need to convert in original RGB\n    #display(PIL.Image.fromarray(cropped_obj_rgb))   # display RGB for troubleshooting\n\n    if save == True:\n        cv2.imwrite(obj_name, cropped_obj_to_np)\n        \n    return cropped_obj_to_np\n\n\ndef SIFTFeatureMatchine(target_obj, all_obj, frameNo):\n    target_obj_bw = cv2.cvtColor(np.array(target_obj), cv2.COLOR_BGR2GRAY)\n    \n    good_match_list = []\n    i=0\n    for current_obj in all_obj:\n        current_obj_bw = cv2.cvtColor(np.array(current_obj), cv2.COLOR_BGR2GRAY)\n        \n        reference_height, reference_width = current_obj_bw.shape\n        # Resize the target image to match the dimensions of the reference image\n        target_obj_bw = cv2.resize(target_obj, (reference_width, reference_height))\n        \n        sift = cv2.SIFT_create()\n        target_obj_keypoints, target_obj_descriptors = sift.detectAndCompute(target_obj_bw, None)\n        current_obj_keypoints, current_obj_descriptors = sift.detectAndCompute(current_obj_bw, None)\n\n        matcher = cv2.BFMatcher()\n        matches = matcher.knnMatch(target_obj_descriptors, current_obj_descriptors, k=2)\n\n        good = []\n        try:  # error handling: ValueError: not enough values to unpack (expected 2, got 1)\n            for m, n in matches:\n                if m.distance < 0.75 * n.distance:\n                    good.append([m])\n\n            good_match_list.append(len(good))\n        except:\n            return -1\n        \n    \n    max_features_matched = max(good_match_list)\n    if max_features_matched>5:   # if no match found\n        target_obj_index = good_match_list.index(max_features_matched)\n        return target_obj_index\n    else:\n        return -1\n\n\n\ndef findTargetObjectIndex(frame, results, target_obj_img, frameNo, obj_dir=''):\n    xyxys = results.boxes.xyxy\n    \n    if len(obj_dir)>0:\n        save = True\n    else:\n        save = False\n    \n    crop_obj_list = []\n    for i in range(len(results)):\n        bbox = (int(xyxys[i][0]), int(xyxys[i][1]), int(xyxys[i][2]), int(xyxys[i][3]))\n\n        obj_name= f'{obj_dir}/Obj_{i}.jpg'\n        crop_obj = crop(frame, bbox, save, obj_name)\n        \n        crop_obj_list.append(crop_obj)\n\n    if(target_obj_img.size != 0 and len(crop_obj_list)>0):\n        target_obj_index = SIFTFeatureMatchine(target_obj_img, crop_obj_list, frameNo)\n        print('FrameNo:',frameNo, ' Target Index: ', target_obj_index)\n        \n        if target_obj_index == -1:\n            return np.array([[0, 0, 0, 0, 0, 0, 0]])\n        else:\n            return results[target_obj_index]\n        \n    else:\n        return results","metadata":{"execution":{"iopub.status.busy":"2024-06-01T16:09:43.614630Z","iopub.execute_input":"2024-06-01T16:09:43.614930Z","iopub.status.idle":"2024-06-01T16:09:43.631119Z","shell.execute_reply.started":"2024-06-01T16:09:43.614904Z","shell.execute_reply":"2024-06-01T16:09:43.630239Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"        '''\n        # Keypoint matching visualization\n        if frameNo in [317, 332, 350, 356, 357]:\n            final_img = cv2.drawMatchesKnn(target_obj_bw,target_obj_keypoints,current_obj_bw,current_obj_keypoints,good,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n            display(PIL.Image.fromarray(final_img))\n            print('FrameNo:',frameNo, ' ObjNo: ', i)\n            fileName = out_dir + 'Diagnosis/Frame_No_' + str(frameNo) + 'Object_No_' + str(i)\n            cv2.imwrite(fileName + '.jpg', final_img)\n            i+=1\n        '''","metadata":{"execution":{"iopub.status.busy":"2024-06-01T16:09:43.632296Z","iopub.execute_input":"2024-06-01T16:09:43.632616Z","iopub.status.idle":"2024-06-01T16:09:43.648680Z","shell.execute_reply.started":"2024-06-01T16:09:43.632579Z","shell.execute_reply":"2024-06-01T16:09:43.647762Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"\"\\n# Keypoint matching visualization\\nif frameNo in [317, 332, 350, 356, 357]:\\n    final_img = cv2.drawMatchesKnn(target_obj_bw,target_obj_keypoints,current_obj_bw,current_obj_keypoints,good,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\\n    display(PIL.Image.fromarray(final_img))\\n    print('FrameNo:',frameNo, ' ObjNo: ', i)\\n    fileName = out_dir + 'Diagnosis/Frame_No_' + str(frameNo) + 'Object_No_' + str(i)\\n    cv2.imwrite(fileName + '.jpg', final_img)\\n    i+=1\\n\""},"metadata":{}}]},{"cell_type":"markdown","source":"# Testing DeepOCSORT","metadata":{}},{"cell_type":"code","source":"#help(DeepOCSORT)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T06:59:55.240003Z","iopub.execute_input":"2024-06-01T06:59:55.240293Z","iopub.status.idle":"2024-06-01T06:59:55.250476Z","shell.execute_reply.started":"2024-06-01T06:59:55.240267Z","shell.execute_reply":"2024-06-01T06:59:55.249239Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\ntracemalloc.start()","metadata":{"execution":{"iopub.status.busy":"2024-06-01T06:59:55.251877Z","iopub.execute_input":"2024-06-01T06:59:55.252268Z","iopub.status.idle":"2024-06-01T06:59:55.264053Z","shell.execute_reply.started":"2024-06-01T06:59:55.252219Z","shell.execute_reply":"2024-06-01T06:59:55.261436Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tracker = DeepOCSORT(\n    model_weights=Path('osnet_x0_25_msmt17.pt'), # which ReID model to use\n    device=device,\n    fp16=fp16\n)\nvid = cv2.VideoCapture(source)\nwriter = create_video_writer(vid, out_dir + \"DeepOCSORT.mp4\")\n\ntracker_predictions = []\n\nframe_no = 1\nwhile True:       \n    clear_output(wait=False)\n    ret, im = vid.read()\n\n    # if video frames end\n    if not ret:\n        break\n    else:\n        results = model.predict(im)[0]\n\n    if np.array(results.boxes.data.tolist()).ndim < 2:\n        continue\n\n    new_results = findTargetObjectIndex(im, results, target_obj_img, frame_no, obj_dir='')\n    try:\n        ts = tracker.update(np.array(new_results.boxes.data.tolist()), im) # --> (x, y, x, y, id, conf, cls)\n    except:\n        ts = new_results\n        \n    xyxys = ts[:,0:4].astype('int') # float64 to int\n    ids = ts[:, 4].astype('int') # float64 to int\n    confs = np.round(ts[:, 5], 2)\n    clss = ts[:, 6]\n\n    # print bboxes with their associated id, cls and conf\n    if ts.shape[0] != 0:\n        for xyxy, id, conf, cls in zip(xyxys, ids, confs, clss):\n            im = cv2.rectangle(\n                im,\n                (xyxy[0], xyxy[1]),\n                (xyxy[2], xyxy[3]),\n                color,\n                thickness\n            )\n            cv2.putText(\n                im,\n                f'id: {id}, conf: {conf}, c: {cls}',\n                (xyxy[0], xyxy[1]-10),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                fontscale,\n                color,\n                thickness\n            )\n            \n            # save tracker predictions for benchmark \n            # <frame_no>, <tracke_obj_id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <conf>, -1 -1 -1\n            # <frame_no>, <tracke_obj_id>, <x1>, <y1>, <x2-x1>, <y2-y1>, <conf>, -1 -1 -1\n            pred = f\"{frame_no}, {id}, {xyxy[0]}, {xyxy[1]}, {xyxy[2]-xyxy[0]}, {xyxy[2]-xyxy[0]}, {conf}, -1, -1, -1\" \n            tracker_predictions.append(pred)\n            frame_no += 1\n\n    # show the frame to our screen\n\n    #writer.write(im)\n\n\n#vid.release()\n#writer.release()\n\nwith open(out_dir + \"DeepOCSORT_pred.txt\", 'w') as mot_file:\n    for pred in tracker_predictions:\n            mot_file.write(pred + '\\n')\n            \n            \nprint(\"Task Completed\")","metadata":{"execution":{"iopub.status.busy":"2024-06-01T07:12:42.081155Z","iopub.execute_input":"2024-06-01T07:12:42.081803Z","iopub.status.idle":"2024-06-01T07:14:40.992387Z","shell.execute_reply.started":"2024-06-01T07:12:42.081744Z","shell.execute_reply":"2024-06-01T07:14:40.990880Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Task Completed\n","output_type":"stream"}]},{"cell_type":"code","source":"current, peak = tracemalloc.get_traced_memory()\ntracemalloc.stop()\n\n# Calculate the time taken\nend_time = time.time()\nexecution_time = end_time - start_time\n\nExecution_time = np.round(execution_time,2)\nCurrent_memory = np.round(current / 1024 / 1024, 2)\nPeak_memory = np.round(peak / 1024 / 1024, 2)\n\nperformance_DeepOCSORT = [Execution_time, Current_memory, Peak_memory]\n\nprint(f\"Execution time: {Execution_time} seconds\")\nprint(f\"Current memory usage: {Current_memory} MB\")\nprint(f\"Peak memory usage: {Peak_memory} MB\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T05:03:04.301940Z","iopub.execute_input":"2024-05-20T05:03:04.302416Z","iopub.status.idle":"2024-05-20T05:03:04.323884Z","shell.execute_reply.started":"2024-05-20T05:03:04.302367Z","shell.execute_reply":"2024-05-20T05:03:04.322853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing StrongSORT\n","metadata":{}},{"cell_type":"code","source":"#help(StrongSORT)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T05:03:04.324926Z","iopub.execute_input":"2024-05-20T05:03:04.325167Z","iopub.status.idle":"2024-05-20T05:03:04.337594Z","shell.execute_reply.started":"2024-05-20T05:03:04.325146Z","shell.execute_reply":"2024-05-20T05:03:04.336795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\ntracemalloc.start()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T05:03:04.338717Z","iopub.execute_input":"2024-05-20T05:03:04.339009Z","iopub.status.idle":"2024-05-20T05:03:04.348986Z","shell.execute_reply.started":"2024-05-20T05:03:04.338985Z","shell.execute_reply":"2024-05-20T05:03:04.347596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tracker = StrongSORT(\n    model_weights=Path('mobilenetv2_x1_4_dukemtmcreid.pt'), # which ReID model to use\n    device=device,\n    fp16=fp16,\n)\ntracker.n_init = 1\nvid = cv2.VideoCapture(source)\nwriter = create_video_writer(vid, out_dir +\"StrongSort.mp4\")\n\ntracker_predictions = []\n\nframe_no = 1\nwhile True:\n    clear_output(wait=False)\n    ret, im = vid.read()\n\n    # if video frames end\n    if not ret:\n        break\n    else:\n        #im_resized = cv2.resize(im, (640, 640))\n        results = model.predict(im)[0]\n\n    if np.array(results.boxes.data.tolist()).ndim < 2:\n        continue\n\n    new_results = findTargetObjectIndex(im, results, target_obj_img, frame_no, obj_dir='')\n    try:\n        ts = tracker.update(np.array(new_results.boxes.data.tolist()), im) # --> (x, y, x, y, id, conf, cls)\n        if (ts.size == 0):\n            ts = np.array( [[0,0,0,0,0,0,0]] )\n    except:\n        ts = np.array( [[0,0,0,0,0,0,0]] )\n\n    xyxys = ts[:,0:4].astype('int') # float64 to int\n    ids = ts[:, 4].astype('int') # float64 to int\n    confs = np.round(ts[:, 5])\n    clss = ts[:, 6]\n\n    # print bboxes with their associated id, cls and conf\n    if ts.shape[0] != 0:\n        for xyxy, id, conf, cls in zip(xyxys, ids, confs, clss):\n            im = cv2.rectangle(\n                im,\n                (xyxy[0], xyxy[1]),\n                (xyxy[2], xyxy[3]),\n                color,\n                thickness\n            )\n            cv2.putText(\n                im,\n                f'id: {id}, conf: {conf}, c: {cls}',\n                (xyxy[0], xyxy[1]-10),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                fontscale,\n                color,\n                thickness\n            )\n            # save tracker predictions for benchmark \n            # <frame_no>, <tracke_obj_id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <conf>, -1 -1 -1\n            # <frame_no>, <tracke_obj_id>, <x1>, <y1>, <x2-x1>, <y2-y1>, <conf>, -1 -1 -1\n            pred = f\"{frame_no}, {id}, {xyxy[0]}, {xyxy[1]}, {xyxy[2]-xyxy[0]}, {xyxy[2]-xyxy[0]}, {conf}, -1, -1, -1\" \n            tracker_predictions.append(pred)\n            frame_no += 1\n\n    # show the frame to our screen\n    \n    #writer.write(im)\n\n\n#vid.release()\n#writer.release()\n\nwith open(out_dir + \"StrongSORT_pred.txt\", 'w') as mot_file:\n    for pred in tracker_predictions:\n            mot_file.write(pred + '\\n')\n            \nprint(\"Task Completed\")","metadata":{"_uuid":"7da1b674-3a9e-4834-9e7d-a7b5782cf2f2","_cell_guid":"bcc3df86-465d-436d-851a-827e603d68a9","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-01T16:10:12.771899Z","iopub.execute_input":"2024-06-01T16:10:12.772321Z","iopub.status.idle":"2024-06-01T16:12:04.287798Z","shell.execute_reply.started":"2024-06-01T16:10:12.772286Z","shell.execute_reply":"2024-06-01T16:12:04.286909Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Task Completed\n","output_type":"stream"}]},{"cell_type":"code","source":"current, peak = tracemalloc.get_traced_memory()\ntracemalloc.stop()\n\n# Calculate the time taken\nend_time = time.time()\nexecution_time = end_time - start_time\n\nExecution_time = np.round(execution_time,2)\nCurrent_memory = np.round(current / 1024 / 1024, 2)\nPeak_memory = np.round(peak / 1024 / 1024, 2)\nperformance_StrongSORT = [Execution_time, Current_memory, Peak_memory]\n\nprint(f\"Execution time: {Execution_time} seconds\")\nprint(f\"Current memory usage: {Current_memory} MB\")\nprint(f\"Peak memory usage: {Peak_memory} MB\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T05:07:28.007057Z","iopub.execute_input":"2024-05-20T05:07:28.007574Z","iopub.status.idle":"2024-05-20T05:07:28.026712Z","shell.execute_reply.started":"2024-05-20T05:07:28.007496Z","shell.execute_reply":"2024-05-20T05:07:28.025660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing BoTSORT","metadata":{}},{"cell_type":"code","source":"#help(BoTSORT)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T05:07:28.028116Z","iopub.execute_input":"2024-05-20T05:07:28.028773Z","iopub.status.idle":"2024-05-20T05:07:28.040836Z","shell.execute_reply.started":"2024-05-20T05:07:28.028740Z","shell.execute_reply":"2024-05-20T05:07:28.039979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\ntracemalloc.start()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T05:07:28.041848Z","iopub.execute_input":"2024-05-20T05:07:28.042126Z","iopub.status.idle":"2024-05-20T05:07:28.052046Z","shell.execute_reply.started":"2024-05-20T05:07:28.042103Z","shell.execute_reply":"2024-05-20T05:07:28.050422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from boxmot import BoTSORT\ntracker = BoTSORT(\n    model_weights=Path('osnet_x0_25_msmt17.pt'),\n    device=device,\n    fp16=fp16,\n)\nvid = cv2.VideoCapture(source)\nwriter = create_video_writer(vid, out_dir + \"BoTSORT.mp4\")\n\ntracker_predictions = []\n\nframe_no = 1\nwhile True:\n    clear_output(wait=False)\n    ret, im = vid.read()\n\n    # if video frames end\n    if not ret:\n        break\n    else:\n        #im_resized = cv2.resize(im, (640, 640))\n        results = model.predict(im)[0]\n\n    if np.array(results.boxes.data.tolist()).ndim < 2:\n        continue\n\n    new_results = findTargetObjectIndex(im, results, target_obj_img, frame_no, obj_dir='')\n    try:\n        ts = tracker.update(np.array(new_results.boxes.data.tolist()), im) # --> (x, y, x, y, id, conf, cls)\n        if (ts.size == 0):\n            ts = np.array( [[0,0,0,0,0,0,0]] )\n    except:\n        ts = np.array( [[0,0,0,0,0,0,0]] )\n        \n\n    xyxys = ts[:,0:4].astype('int') # float64 to int\n    ids = ts[:, 4].astype('int') # float64 to int \n    confs = np.round(ts[:, 5])\n    clss = ts[:, 6]\n\n    # print bboxes with their associated id, cls and conf\n    if ts.shape[0] != 0:\n        for xyxy, id, conf, cls in zip(xyxys, ids, confs, clss):\n            im = cv2.rectangle(\n                im,\n                (xyxy[0], xyxy[1]),\n                (xyxy[2], xyxy[3]),\n                color,\n                thickness\n            )\n            cv2.putText(\n                im,\n                f'id: {id}, conf: {conf}, c: {cls}',\n                (xyxy[0], xyxy[1]-10),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                fontscale,\n                color,\n                thickness\n            )\n            # save tracker predictions for benchmark \n            # <frame_no>, <tracke_obj_id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <conf>, -1 -1 -1\n            # <frame_no>, <tracke_obj_id>, <x1>, <y1>, <x2-x1>, <y2-y1>, <conf>, -1 -1 -1\n            pred = f\"{frame_no}, {id}, {xyxy[0]}, {xyxy[1]}, {xyxy[2]-xyxy[0]}, {xyxy[2]-xyxy[0]}, {conf}, -1, -1, -1\" \n            tracker_predictions.append(pred)\n            frame_no += 1\n\n    # show the frame to our screen\n    \n    #writer.write(im)\n\n\n#vid.release()\n#writer.release()\n\nwith open(out_dir + \"BoTSORT_pred.txt\", 'w') as mot_file:\n    for pred in tracker_predictions:\n            mot_file.write(pred + '\\n')\n            \nprint(\"Task Completed\")","metadata":{"_uuid":"d9593daa-bbb5-42b4-8dd8-cedf003536c2","_cell_guid":"6b06d5c4-ee5b-4a67-bcf8-1952810da9e5","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-01T16:14:33.443900Z","iopub.execute_input":"2024-06-01T16:14:33.444861Z","iopub.status.idle":"2024-06-01T16:16:30.762285Z","shell.execute_reply.started":"2024-06-01T16:14:33.444808Z","shell.execute_reply":"2024-06-01T16:16:30.761296Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Task Completed\n","output_type":"stream"}]},{"cell_type":"code","source":"current, peak = tracemalloc.get_traced_memory()\ntracemalloc.stop()\n\n# Calculate the time taken\nend_time = time.time()\nexecution_time = end_time - start_time\n\nExecution_time = np.round(execution_time,2)\nCurrent_memory = np.round(current / 1024 / 1024, 2)\nPeak_memory = np.round(peak / 1024 / 1024, 2)\n\nperformance_BoTSORT = [Execution_time, Current_memory, Peak_memory]\n\nprint(f\"Execution time: {Execution_time} seconds\")\nprint(f\"Current memory usage: {Current_memory} MB\")\nprint(f\"Peak memory usage: {Peak_memory} MB\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T05:12:06.396333Z","iopub.execute_input":"2024-05-20T05:12:06.397246Z","iopub.status.idle":"2024-05-20T05:12:06.419538Z","shell.execute_reply.started":"2024-05-20T05:12:06.397194Z","shell.execute_reply":"2024-05-20T05:12:06.418487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing BYTETracker","metadata":{}},{"cell_type":"code","source":"#help(BYTETracker)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T05:12:06.420850Z","iopub.execute_input":"2024-05-20T05:12:06.421208Z","iopub.status.idle":"2024-05-20T05:12:06.434153Z","shell.execute_reply.started":"2024-05-20T05:12:06.421172Z","shell.execute_reply":"2024-05-20T05:12:06.433331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\ntracemalloc.start()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T05:12:06.435461Z","iopub.execute_input":"2024-05-20T05:12:06.435807Z","iopub.status.idle":"2024-05-20T05:12:06.445577Z","shell.execute_reply.started":"2024-05-20T05:12:06.435783Z","shell.execute_reply":"2024-05-20T05:12:06.444010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from boxmot import BYTETracker\ntracker =  BYTETracker()\nvid = cv2.VideoCapture(source)\nwriter = create_video_writer(vid, out_dir+\"BYTETracker.mp4\")\n\ntracker_predictions = []\n\nframe_no = 1\nwhile True:\n    clear_output(wait=False)\n    ret, im = vid.read()\n\n    # if video frames end\n    if not ret:\n        break\n    else:\n        #im_resized = cv2.resize(im, (640, 640))\n        results = model.predict(im)[0]\n\n    if np.array(results.boxes.data.tolist()).ndim < 2:\n        continue\n\n    new_results = findTargetObjectIndex(im, results, target_obj_img, frame_no, obj_dir='')\n    try:\n        ts = tracker.update(np.array(new_results.boxes.data.tolist()), im) # --> (x, y, x, y, id, conf, cls)\n        if (ts.size == 0):\n            ts = np.array( [[0,0,0,0,0,0,0]] )\n    except:\n        ts = np.array( [[0,0,0,0,0,0,0]] )\n        \n        \n    xyxys = ts[:,0:4].astype('int') # float64 to int\n    ids = ts[:, 4].astype('int') # float64 to int\n    confs = np.round(ts[:, 5])\n    clss = ts[:, 6]\n\n    # print bboxes with their associated id, cls and conf\n    if ts.shape[0] != 0:\n        for xyxy, id, conf, cls in zip(xyxys, ids, confs, clss):\n            im = cv2.rectangle(\n                im,\n                (xyxy[0], xyxy[1]),\n                (xyxy[2], xyxy[3]),\n                color,\n                thickness\n            )\n            cv2.putText(\n                im,\n                f'id: {id}, conf: {conf}, c: {cls}',\n                (xyxy[0], xyxy[1]-10),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                fontscale,\n                color,\n                thickness\n            )\n            # save tracker predictions for benchmark \n            # <frame_no>, <tracke_obj_id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <conf>, -1 -1 -1\n            # <frame_no>, <tracke_obj_id>, <x1>, <y1>, <x2-x1>, <y2-y1>, <conf>, -1 -1 -1\n            pred = f\"{frame_no}, {id}, {xyxy[0]}, {xyxy[1]}, {xyxy[2]-xyxy[0]}, {xyxy[2]-xyxy[0]}, {conf}, -1, -1, -1\" \n            tracker_predictions.append(pred)\n            frame_no += 1\n\n    # show the frame to our screen\n    \n    #writer.write(im)\n\n\n#vid.release()\n#writer.release()\n\nwith open(out_dir + \"ByteTracker_pred.txt\", 'w') as mot_file:\n    for pred in tracker_predictions:\n            mot_file.write(pred + '\\n')\n            \nprint(\"Task Completed\")","metadata":{"_uuid":"01eb20ab-a0fe-4065-9ef5-3fc610f9ddf9","_cell_guid":"4eef3134-fe31-4173-b389-cceb0e6281dd","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-01T16:16:30.764284Z","iopub.execute_input":"2024-06-01T16:16:30.765163Z","iopub.status.idle":"2024-06-01T16:18:09.911361Z","shell.execute_reply.started":"2024-06-01T16:16:30.765124Z","shell.execute_reply":"2024-06-01T16:18:09.910475Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Task Completed\n","output_type":"stream"}]},{"cell_type":"code","source":"current, peak = tracemalloc.get_traced_memory()\ntracemalloc.stop()\n\n# Calculate the time taken\nend_time = time.time()\nexecution_time = end_time - start_time\n\nExecution_time = np.round(execution_time,2)\nCurrent_memory = np.round(current / 1024 / 1024, 2)\nPeak_memory = np.round(peak / 1024 / 1024, 2)\n\nperformance_BYTETracker = [Execution_time, Current_memory, Peak_memory]\n\nprint(f\"Execution time: {Execution_time} seconds\")\nprint(f\"Current memory usage: {Current_memory} MB\")\nprint(f\"Peak memory usage: {Peak_memory} MB\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T05:16:00.288486Z","iopub.execute_input":"2024-05-20T05:16:00.289027Z","iopub.status.idle":"2024-05-20T05:16:00.306772Z","shell.execute_reply.started":"2024-05-20T05:16:00.288975Z","shell.execute_reply":"2024-05-20T05:16:00.305721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-05-20T05:16:00.308276Z","iopub.execute_input":"2024-05-20T05:16:00.308690Z","iopub.status.idle":"2024-05-20T05:16:00.322387Z","shell.execute_reply.started":"2024-05-20T05:16:00.308650Z","shell.execute_reply":"2024-05-20T05:16:00.321405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(list(zip(performance_DeepOCSORT, performance_StrongSORT, performance_BoTSORT, performance_BYTETracker)),\n                  columns = ['DeepOCSORT', 'StrongSORT', 'BoTSORT', 'BYTETracker'],\n                  index = ['Execution_time(s)', 'Current_memory(MB)', 'Peak_memory(MB)'])\n\ndf.to_csv(out_dir + 'SimulationPerformance.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-20T05:16:00.323730Z","iopub.execute_input":"2024-05-20T05:16:00.324148Z","iopub.status.idle":"2024-05-20T05:16:00.339661Z","shell.execute_reply.started":"2024-05-20T05:16:00.324113Z","shell.execute_reply":"2024-05-20T05:16:00.338500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\n\nfor i in [4,5,6]:\n    OUTPUT_NAME = f'/kaggle/working/TrackResultsTest{i}'\n    DIRECTORY_TO_ZIP = f'/kaggle/working/Test{i}'\n    \n    print(DIRECTORY_TO_ZIP)\n    shutil.make_archive(OUTPUT_NAME, 'zip', DIRECTORY_TO_ZIP)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T05:58:52.688826Z","iopub.execute_input":"2024-05-20T05:58:52.689229Z","iopub.status.idle":"2024-05-20T06:01:32.247353Z","shell.execute_reply.started":"2024-05-20T05:58:52.689167Z","shell.execute_reply":"2024-05-20T06:01:32.245778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tracker Comparisons\nhttps://pramod-atre.medium.com/understanding-object-tracking-a-hands-on-approach-part-1-3fb1afd0ae46","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/JonathonLuiten/TrackEval.git\n","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:32:26.941295Z","iopub.execute_input":"2024-06-01T14:32:26.941715Z","iopub.status.idle":"2024-06-01T14:32:27.975251Z","shell.execute_reply.started":"2024-06-01T14:32:26.941664Z","shell.execute_reply":"2024-06-01T14:32:27.974063Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"fatal: destination path 'TrackEval' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}